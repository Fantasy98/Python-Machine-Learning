import numpy as np
import tensorflow as tf 
from tensorflow import keras
from keras import layers

#############################################################################################
## Positional Embedding which contains the embedding of words and its position
#############################################################################################
class PositionalEmbedding(layers.Layer):
    def __init__(self,sequence_length,input_dim,output_dim, **kwargs):
        super().__init__(**kwargs)

        # Embedding for words
        self.token_embeddings = layers.Embedding(input_dim=input_dim,output_dim=output_dim)

        #Embedding for positions
        self.position_embeddings = layers.Embedding(input_dim=sequence_length,output_dim=output_dim)

        self.sequence_length = sequence_length
        self.input_dim = input_dim
        self.output_dim = output_dim
    
    def call(self,inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0,limit=length,delta=1)
        embedded_tokens =self.token_embeddings(inputs)
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens+embedded_positions
    
    def compute_mask(self,inputs,mask=None):
        return tf.math.not_equal(inputs,0)


    def get_config(self):
        config = super().get_config()
        config.update({
                "output_dim":self.output_dim,
                "input_dim":self.input_dim,
                "sequence_length":self.sequence_length
        })
        return config




class TransformerEncoder(layers.Layer):
    def __init__(self,embed_dim,dense_dim,num_heads,**kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention = layers.MultiHeadAttention(
            num_heads=num_heads,key_dim=embed_dim
        )
        self.dens_proj = keras.Sequential([
            layers.Dense(dense_dim,activation='relu'),
            layers.Dense(embed_dim,)
        ])
        self.layernorm_1 = layers.LayerNormalization() 
        self.layernorm_2 = layers.LayerNormalization()

    def call(self,inputs,mask=None):

        # The mask that will be generated by 
        # the Embedding layer will be 2D, but 
        # the attention layer expects to be 3D 
        # or 4D, so we expand its rank.
        if mask is not None:
            mask= mask[:,tf.newaxis,:]
        
        attention_output = self.attention(inputs,inputs,attention_mask = mask)

        ## 1st Normalization and Residual
        proj_input = self.layernorm_1(inputs + attention_output)

        ## Dense layers
        proj_output = self.dens_proj(proj_input)

        ## 2nd Normalization & Residual
        return self.layernorm_2(proj_input+proj_output)
        
    # Implement 
    # serialization so 
    # we can save the 
    # model.

    def get_config(self):
        config = super().get_config()
        config.update({
            "embed_dim":self.embed_dim,
            "num_heads" : self.num_heads,
            "dense_dim" : self.dense_dim
        })
        return config



class TransformerDecoder(layers.Layer):
    def __init__(self,embed_dim,dense_dim,num_heads,**kwargs):
        super().__init__(**kwargs)

        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads

        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads,key_dim=embed_dim)
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads,key_dim=embed_dim)
        self.dense_proj = keras.Sequential([
            layers.Dense(dense_dim,activation="relu"),
            layers.Dense(embed_dim,),
            
        ])

        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()

        ##layers.Layer.support_masking

        # This attribute ensures that the layer will 
        # propagate its input mask to its outputs; 
        # masking in Keras is explicitly opt-in. If 
        # you pass a mask to a layer that doesn’t 
        # implement compute_mask() and that 
        # _ doesn’t expose this supports_masking 
        # attribute, that’s an error.
        self.supports_masking = True

    def get_config(self):
        config = super().get_config()
        config.update({
            "embed_dim" : self.embed_dim,
            "num_heads": self.num_heads,
            "dense_dim":self.dense_dim,
        })

        return config
    
    def get_causal_attention_mask(self,inputs):
        input_shape = tf.shape(inputs)
        batch_size , sequence_length = input_shape[0],input_shape[1]
        i = tf.range(sequence_length)[:,tf.newaxis]
        j = tf.range(sequence_length)

        mask = tf.cast(i >= j,dtype="int32")
       
        mask = tf.reshape(mask,(1,input_shape[1],input_shape[1]))

        ## mult = [[batch_size,1],
        #             [1,1]]
        mult = tf.concat(
            [tf.expand_dims(batch_size,-1),
                tf.constant([1,1],dtype=tf.int32,)
            ],axis=0
        )

        return tf.tile(mask,mult)
        ## It will repeat mask for batch_size times.
        ## The shape will be like shape= ( batch_size,(mask.shape),1,1,1 )


    def call(self,inputs,encoder_outputs,mask=None):
        causal_mask = self.get_causal_attention_mask(inputs)

        if mask is not  None:
            padding_mask = tf.cast( mask[:,tf.newaxis,:],dtype="int32" )
            padding_mask = tf.minimum(padding_mask,causal_mask)
        
        attent_output_1 = self.attention_1(
            query = inputs,
            value = inputs,
            key = inputs,
            # Causal_mask to restrict by N not N+1
            attention_mask = causal_mask
        )

        ## Residual & Normalization
        attent_output_1 = self.layernorm_1(inputs+attent_output_1)

        attent_output_2 = self.attention_2(
            query = attent_output_1,
            value = encoder_outputs,
            key = encoder_outputs,
            ## padding_mask make sure the previous restriction and the function of padding
            attention_mask = padding_mask,
        )

        ## Residual & Normalization
        attent_output_2 = self.layernorm_2(attent_output_1+attent_output_2)

        ## Two dense layers
        proj_output = self.dense_proj(attent_output_2)

        ##the 3rd layernormalization & Residual 
        return self.layernorm_3(proj_output + attent_output_2)



def Transformer(sequence_length,vocab_size):
    embed_dim = 256
    dense_dim = 2048
    num_heads =8 

    encoder_inputs = keras.Input(shape= (None,),dtype="int64",name="english")
    x = PositionalEmbedding(sequence_length,vocab_size,embed_dim)(encoder_inputs)
    encoder_outputs = TransformerEncoder(embed_dim,dense_dim,num_heads)(x)

    decoder_inputs = keras.Input(shape=(None,),dtype="int64",name='spanish')
    x = PositionalEmbedding(sequence_length,vocab_size,embed_dim)(decoder_inputs)
    x = TransformerDecoder(embed_dim,dense_dim,num_heads)(x,encoder_outputs)


    x = layers.Dropout(0.5)(x)
    decoder_outputs = layers.Dense(vocab_size,activation="softmax")(x)
    transformer = keras.Model([encoder_inputs,decoder_inputs],decoder_outputs)

    return transformer

