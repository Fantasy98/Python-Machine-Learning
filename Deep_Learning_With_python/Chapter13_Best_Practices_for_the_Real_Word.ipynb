{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13 Best Practices for the Real World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1 Getting the most out of your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.1 Hypterparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The process of optimizing hyperparameters typically looks like this:\n",
    "    1. Choose a set of hyperparameters (automatically).\n",
    "\n",
    "    2. Build the corresponding model.\n",
    "    \n",
    "    3. Fit it to your training data, and measure performance on the validation data.\n",
    "    \n",
    "    4. Choose the next set of hyperparameters to try (automatically).\n",
    "    \n",
    "    5. Repeat.\n",
    "    \n",
    "    6. Eventually, measure performance on your test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras TUNER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ KerasTuner lets you replace hard-coded hyperparameter values, such as units=32,\n",
    "with a range of possible choices, such as Int(name=\"units\", min_value=16,\n",
    "max_value=64, step=16). \n",
    "\n",
    "+ This set of choices in a given model is called the search space\n",
    "of the hyperparameter tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify a search space, define a model-building function (see the next listing).\n",
    "It takes an hp argument, from which you can sample hyperparameter ranges, and it\n",
    "returns a compiled Keras model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 13.1 A KerasTuner model-building function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "from kerastuner import HyperParameters as hp\n",
    "\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "#     Sample hyperparameter values from the\n",
    "#  hp object. \n",
    "# After sampling, these values\n",
    "#  (such as the \"units\" variable here) are\n",
    "#  just regular Python constants\n",
    "\n",
    "    units = hp.Int(name=\"units\",min_value=16,max_value =64,step=16)\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(units=units,activation='relu'),\n",
    "        layers.Dense(10,activation='softmax'),\n",
    "    ])\n",
    "\n",
    "    optimizer = hp.Choice(name = \"optimizer\", values = [\"rmsprop\",\"adam\"])\n",
    "\n",
    "    model.compile(optimizer = optimizer,\n",
    "                    loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "                    metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 13.2 A KerasTuner HyperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner as kt\n",
    "\n",
    "\n",
    "class SimpleMLP(kt.HyperModel):\n",
    "    def __init__(self,num_classes):\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def build(self,hp):\n",
    "        units  = hp.Int(name= \"units\",min_value = 16,max_value = 64,step = 16)\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(units=units,activation=\"relu\"),\n",
    "            layers.Dense(self.num_classes,activation='softmax')\n",
    "        ])\n",
    "        optimizer = hp.Choice(name= \"optimizer\",values =['rmsprop','adam'])\n",
    "        model.compile(\n",
    "            optimizer = optimizer,\n",
    "            loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "            metrics = ['accuracy']\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel = SimpleMLP(num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define a __“tuner.”__ Schematically, you can think of a tuner as a for\n",
    "loop that will repeatedly\n",
    "\n",
    "1. Pick a set of hyperparameter values\n",
    "\n",
    "2. Call the model-building function with these values to create a model\n",
    "\n",
    "3. Train the model and record its metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ KerasTuner has several built-in tuners available: $RandomSearch$ , $BayesianOptimization$, and $Hyperband$. \n",
    "\n",
    "\n",
    "+ Let’s try $BayesianOptimization$, a tuner that attempts to make smart predictions for which new hyperparameter values are likely to perform best\n",
    " given the outcomes of previous choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import BayesianOptimization\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "\n",
    "#     Specify the metric that the tuner will seek to \n",
    "# optimize. Always specify validation metrics, \n",
    "# since the goal of the search process is to \n",
    "# find models that generalize!\n",
    "    objective='val_accuracy',\n",
    "\n",
    "#     Maximum number of different \n",
    "# model configurations (“trials”) \n",
    "# to try before ending the search.\n",
    "    max_trials=100,\n",
    "    \n",
    "#     To reduce metrics variance, you can train the \n",
    "#  same model multiple times and average the results. \n",
    "#  executions_per_trial is how many training rounds \n",
    "#  _ _ (executions) to run for each model configuration (trial).\n",
    "    executions_per_trial = 2,\n",
    "\n",
    "   directory = \"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch13\\\\mnist_kt_test\",\n",
    "\n",
    "#    Whether to overwrite data in directory to start a new search. Set this to \n",
    "# True if you’ve modified the model-building function, or to False to resume \n",
    "# a previously started search with the same model-building function.\n",
    "   overwrite = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': None}\n",
      "optimizer (Choice)\n",
      "{'default': 'rmsprop', 'conditions': [], 'values': ['rmsprop', 'adam'], 'ordered': False}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective Maximization and Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For built-in metrics (like accuracy, in our case), the direction of the metric (accuracy\n",
    " should be maximized, but a loss should be minimized) is inferred by KerasTuner.\n",
    " However, for a custom metric, you should specify it yourself, like this:\n",
    "\n",
    " objective = kt.Objective(\n",
    "  name=\"val_accuracy\", \n",
    "  direction=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's launch the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = keras.datasets.mnist.load_data()\n",
    "x_train= x_train.reshape((-1,28*28)).astype(\"float32\")/255\n",
    "x_test= x_test.reshape((-1,28*28)).astype(\"float32\")/255\n",
    "\n",
    "\n",
    "x_train_full = x_train[:]\n",
    "y_train_full = y_train[:]\n",
    "x_test_full = x_test[:]\n",
    "\n",
    "num_val_samples = 10000\n",
    "\n",
    "x_train,x_val = x_train[num_val_samples:],x_train[:num_val_samples]\n",
    "y_train,y_val = y_train[num_val_samples:],y_train[:num_val_samples]\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        patience = 5,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner.search(x_train,y_train,\n",
    "#                 batch_size = 128,\n",
    "#                 epochs = 30,\n",
    "#                 validation_data = (x_val,y_val),\n",
    "#                 callbacks = callbacks,\n",
    "#                 verbose =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, with a typical search\n",
    " space and dataset, you’ll often find yourself letting the hyperparameter search run\n",
    " overnight or even over several days. \n",
    " \n",
    " If your search process crashes, you can always\n",
    " __restart__ it—just __specify overwrite=False__ in the tuner so that it can resume from the\n",
    " trial logs stored on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 13.3 Querying the best hyperparameter configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 4\n",
    "best_hps = tuner.get_best_hyperparameters(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one last parameter to be decided is the __epochs__.\n",
    "\n",
    "Using an aggressive value of __patience__ in the __EarlyStopping()__ saves the best epoch, but it may lead to a __underfit__ model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example of obtain best epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_epoch(hp):\n",
    "    model = build_model(hp)\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor = 'val_loss',\n",
    "            mode = 'min',\n",
    "            ## Note the very high patience value\n",
    "            patience =10\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,y_train,\n",
    "        validation_data  = (x_val,y_val),\n",
    "        epochs = 100,\n",
    "        batch_size =128,\n",
    "        callbacks = callbacks,\n",
    "        \n",
    "    )\n",
    "\n",
    "    val_loss_per_epoch = history.history['val_loss']\n",
    "\n",
    "    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_trained_model(hp):\n",
    "    model  = build_model(hp)\n",
    "    best_epoch = get_best_epoch(hp)\n",
    "    model.fit(\n",
    "        x_train_full,y_train_full,batch_size = 128,\n",
    "        epoch = int(best_epoch*1.2)\n",
    "    \n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_models = []\n",
    "\n",
    "# for hp in best_hps: \n",
    "#     model = get_best_trained_model(hp)\n",
    "#     model.evaluate(x_test,y_test)\n",
    "#     best_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you’re not worried about slightly underperforming, there’s a shortcut you\n",
    " can take: \n",
    " \n",
    " just use the tuner to reload the top-performing models with the best weights\n",
    " saved during the hyperparameter search, without retraining new models from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_models = tuner.get_best_models(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__ that an important issue to think about is when doing automatic hyperparameter optimization at scale is __validation-set__ overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e450050b432e843bda3c41bf3272c133bfc370a7003f3e377e27f87a49ce1127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
