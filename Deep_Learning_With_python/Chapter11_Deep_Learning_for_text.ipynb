{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 Deep Learning for text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Natural language processing(NLP): The bird’s eye view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+   Every machine language was $designed$: its\n",
    " starting point was a human engineer writing down a set of formal rules to describe\n",
    " what statements you could make in that language and what they meant\n",
    "\n",
    "+   Machine-readable language is highly structured and rigorous, using precise syntactic\n",
    " rules to weave together exactly defined concepts from a fixed vocabulary, natural language is messy—ambiguous, chaotic, sprawling, and constantly in flux.\n",
    "\n",
    "+   That’s what modern NLP is about: using machine learning and large datasets to\n",
    " give computers the ability not to understand language, which is a more lofty goal, but\n",
    " to ingest a piece of language as input and return something useful, like predicting the\n",
    " following\n",
    "    1. “What’s the topic of this text?” ($text$ $classification$)\n",
    "    \n",
    "    2. “Does this text contain abuse?” ($content$ $filtering$)\n",
    "    \n",
    "    3. “Does this text sound positive or negative?” ($sentiment$ $analysis$)\n",
    "    \n",
    "    4. “What should be the next word in this incomplete sentence?” ($language$ $modeling$)\n",
    "    \n",
    "    5. “How would you say this in German?” ($translation$)\n",
    "    \n",
    "    6. “How would you summarize this article in one paragraph?” ($summarization$)\n",
    "    \n",
    "    7. etc.\n",
    "\n",
    "+  they simply\n",
    " look for statistical regularities in their input data, which turns out to be sufficient to\n",
    " perform well on many simple tasks. In much the same way that computer vision is pattern recognition applied to pixels, NLP is pattern recognition applied to words, sentences, and paragraphs\n",
    "\n",
    "\n",
    "+   Finally, around 2017–2018, a new architecture rose to replace RNNs: the __Transformer__, which you will learn about in the second half of this chapter. Transformers\n",
    " unlocked considerable progress across the field in a short period of time, and today\n",
    " most NLP systems are based on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Preparing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Vectorizing$ text is the process of transforming text into numeric tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $Standardize$:  First, you standardize the text to make it easier to process, such as by converting\n",
    " it to lowercase or removing punctuation.\n",
    "\n",
    "2. $Tokenization$:  You split the text into units (called tokens), such as characters, words, or groups\n",
    " of words. This is called tokenization.  \n",
    " \n",
    "3. $One-Hot-Encode$:  You convert each such token into a numerical vector. This will usually involve\n",
    " first indexing all tokens present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.1 Text standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text standardization is a basic form of feature engineering that aims to __erase\n",
    " encoding differences that you don’t want your model to have to deal with__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  __Convert to lowercase and remove punctuation characters__.\n",
    "    \n",
    "\n",
    "2.  __Stemming__: converting variations of a term (such as different conjugated forms of a verb) into a single shared representation.\n",
    "    “caught” and “been catching” into “[catch]” or “cats” into “[cat]”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.2 Text splitting (tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Word-level tokenization__ —Where tokens are space-separated (or punctuation-separated) substrings. A variant of this is to further split words into subwords\n",
    "when applicable—for instance, treating “staring” as “star+ing” or “called” as\n",
    "“call+ed.”\n",
    "\n",
    "2. __N-gram tokenization__ —Where tokens are groups of N consecutive words. For\n",
    "instance, “the cat” or “he was” would be 2-gram tokens (also called bigrams).\n",
    "\n",
    "\n",
    "3. __Character-level tokenization__ —Where each character is its own token. In practice,\n",
    "this scheme is rarely used, and you only really see it in specialized contexts, like\n",
    "text generation or speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two kinds of text-processing models: \n",
    "1. $Sequence$  $model$ :care about word __order__\n",
    "\n",
    "2. $Bag-of-words$  $model$: treat input words as a set, __discarding their original order__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.3 Vocabulary indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your text is split into tokens, you need to encode each token into a numerical\n",
    " representation.\n",
    "\n",
    "the way you’d go about it is to build\n",
    " an index of all terms found in the training data (the “vocabulary”), and assign a\n",
    " unique integer to each entry in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary = {}\n",
    "# for text in dataset: \n",
    "#     text = standardize(text)\n",
    "#     tokens = tokenize(text)\n",
    "#     for token in tokens:\n",
    "#         if token is not in vocabulary:\n",
    "#             vocabulary[token] = len(vocabulary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then convert that integer into a vector encoding that can be processed by a\n",
    "neural network, like a one-hot vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# def one_hot_encode_token(token):\n",
    "#     vetcor = np.zeros((len(vocabulary),))\n",
    "#     token_index = vocabulary[token]\n",
    "#     vetcor[token_index]\n",
    "#     return vetcor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that at this step it’s common to restrict the vocabulary to only the top 20,000 or\n",
    " 30,000 most common words found in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data you were using from keras.datasets.imdb was\n",
    " already preprocessed into sequences of integers, where each integer stood for a given\n",
    " word. \n",
    " \n",
    "Back then, we used the setting num_words=10000, in order to restrict our vocabulary to the __top 10,000__ most common words found in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we look up a new token in our vocabulary index, it may not necessarily exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your training data may not have contained any instance of the word “cherimoya” (or maybe you\n",
    " excluded it from your index because it was too rare), so doing token_index =\n",
    " vocabulary[\"cherimoya\"] may result in a KeyError. \n",
    " \n",
    "To handle this, you should use\n",
    " an __“out of vocabulary” index__ (abbreviated as $OOV$ index)—a catch-all for any token\n",
    " that wasn’t in the index. \n",
    " \n",
    "It’s usually __index 1__: you’re actually doing token_index =\n",
    " vocabulary.get(token, 1). When decoding a sequence of integers back into words,\n",
    " you’ll replace 1 with something like “[UNK]” (which you’d call an “OOV token”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.4 Using the TextVectorization layer\n",
    "\n",
    "Every step I’ve introduced so far would be very easy to implement in pure Python.\n",
    "Maybe you could write something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self,text:str) -> str:\n",
    "        text = text.lower()\n",
    "        return\"\".join( char for char in text if char not in string.punctuation )\n",
    "\n",
    "    def tokenize(self,text:str)->list:\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "    \n",
    "    def make_vocabulary(self,dataset):\n",
    "        self.vocabulary = {\"\":0,\"[UNK]\":1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens :\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_volcabulary = dict ( (val,key) for key,val in self.vocabulary.items()   )\n",
    "    \n",
    "    def encode(self,text:str) -> list:\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token,1) for token in tokens ]\n",
    "\n",
    "    def decode(self,int_sequence) -> str:\n",
    "        return \"\".join(\n",
    "            self.inverse_volcabulary.get(i,\"[UNK]\") for i in int_sequence\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, '[UNK]': 1, 'i': 2, 'write': 3, 'erase': 4, 'rewrite': 5, 'again': 6, 'and': 7, 'then': 8, 'a': 9, 'poppy': 10, 'blooms': 11}\n"
     ]
    }
   ],
   "source": [
    "vector = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\"\n",
    "]\n",
    "\n",
    "vector.make_vocabulary(dataset)\n",
    "print(vector.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentence:\n",
      "[2, 3, 5, 7, 1, 5, 6]\n",
      "Decoded Sentence\n",
      "iwriterewriteand[UNK]rewriteagain\n"
     ]
    }
   ],
   "source": [
    "test_sequence = \"I write, rewrite, and still rewrite again.\"\n",
    "encoded_sentence = vector.encode(test_sequence)\n",
    "print(f\"Encoded Sentence:\\n{encoded_sentence}\")\n",
    "\n",
    "decoded_sentence = vector.decode(encoded_sentence)\n",
    "print(f\"Decoded Sentence\\n{decoded_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vect = CountVectorizer().fit_transform(vector.vocabulary) \n",
    "# vect = vect.toarray()\n",
    "# print(vect)\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# one = OneHotEncoder().fit_transform(vect)\n",
    "# print(one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, using something like this wouldn’t be very performant. \n",
    "\n",
    "In practice, you’ll work with the Keras __TextVectorization__ layer, which is fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "# Configures the layer to return sequences of words encoded \n",
    "# as integer indices. There are several other output modes \n",
    "# available, which you will see in action in a bit.\n",
    "text_vectorization = TextVectorization(output_mode=\"int\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the TextVectorization layer will use the setting\n",
    "+ __convert to lowercase__ and __remove punctuation__ for text $standardization$, \n",
    "+  __split on whitespace__ for $tokenization$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that\n",
    " such custom functions should operate on __tf.string tensors__, not regular Python\n",
    " strings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor:tf.strings) -> tf.strings:\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\",\"\"\n",
    "    )\n",
    "\n",
    "def custom_split_fn(string_tensor:tf.strings) -> tf.strings:\n",
    "    return tf.strings.split(string_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    output_mode='int',\n",
    "    standardize= custom_standardization_fn,\n",
    "    split= custom_split_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To index the vocabulary of a text corpus, just call the __adapt() method__ of the layer\n",
    "with a Dataset object that yields __strings__, or just with __a list of Python strings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\"\n",
    "]\n",
    "\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can __retrieve the computed vocabulary__ via __get_vocabulary()__—this can\n",
    " be useful if you need to convert text encoded as integer sequences back into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstartion, let's try to encode and  decode the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.1 Displaying the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
      "iwriterewriteand[UNK]rewriteagain\n"
     ]
    }
   ],
   "source": [
    "vocalbuary= text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)\n",
    "\n",
    "inverse_volcab= dict( enumerate(vocalbuary))\n",
    "decoded_sentence = \"\".join( inverse_volcab[int(i)] \n",
    "                            for i in encoded_sentence\n",
    "    )\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the __TextVectorization__ layer in a __tf.data__ pipeline or as part of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  __TextVectorization__ __ONLY__ works on __CPU__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are __Two__ ways to use __TextVectorization__ layer\n",
    "\n",
    "1.  Put it in the __tf.data__ pipeline:\n",
    "    \n",
    "    + __int_sequence_dataset = string_dataset.map( \n",
    "            text_vectorization,\n",
    "            num_parallel_calls=4)__\n",
    "    \n",
    "    +  while the GPU runs the model on one batch of vectorized data, the CPU stays busy by vectorizing the next batch of\n",
    "        raw strings.\n",
    "    \n",
    "    + __Recommands for GPU version__\n",
    "\n",
    "\n",
    "\n",
    "2.  Make it part of the model:\n",
    "    +   __text_input = keras.Input(shape=(), dtype=\"string\")__\n",
    "\n",
    "        __vectorized_text = text_vectorization(text_input)__\n",
    "\n",
    "        __embedded_input = keras.layers.Embedding(...)(vectorized_text)__\n",
    "\n",
    "        __output = ...__\n",
    "\n",
    "        __model = keras.Model(text_input, output)__\n",
    "\n",
    "    +   This means that\n",
    "        at each training step, the rest of the model (placed on the GPU) will have to wait for\n",
    "        the output of the TextVectorization layer (placed on the CPU) to be ready in order\n",
    "        to get to work\n",
    "\n",
    "    +   __Not suitable for GPU version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, the TextVectorization layer enables you to include\n",
    " text preprocessing right into your model, making it easier to deploy—even if you were\n",
    " originally using the layer as part of a tf.data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Two approaches for representing groups of words: Sets and Sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e450050b432e843bda3c41bf3272c133bfc370a7003f3e377e27f87a49ce1127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
