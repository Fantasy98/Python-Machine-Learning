{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 Deep Learning for text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Natural language processing(NLP): The bird’s eye view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+   Every machine language was $designed$: its\n",
    " starting point was a human engineer writing down a set of formal rules to describe\n",
    " what statements you could make in that language and what they meant\n",
    "\n",
    "+   Machine-readable language is highly structured and rigorous, using precise syntactic\n",
    " rules to weave together exactly defined concepts from a fixed vocabulary, natural language is messy—ambiguous, chaotic, sprawling, and constantly in flux.\n",
    "\n",
    "+   That’s what modern NLP is about: using machine learning and large datasets to\n",
    " give computers the ability not to understand language, which is a more lofty goal, but\n",
    " to ingest a piece of language as input and return something useful, like predicting the\n",
    " following\n",
    "    1. “What’s the topic of this text?” ($text$ $classification$)\n",
    "    \n",
    "    2. “Does this text contain abuse?” ($content$ $filtering$)\n",
    "    \n",
    "    3. “Does this text sound positive or negative?” ($sentiment$ $analysis$)\n",
    "    \n",
    "    4. “What should be the next word in this incomplete sentence?” ($language$ $modeling$)\n",
    "    \n",
    "    5. “How would you say this in German?” ($translation$)\n",
    "    \n",
    "    6. “How would you summarize this article in one paragraph?” ($summarization$)\n",
    "    \n",
    "    7. etc.\n",
    "\n",
    "+  they simply\n",
    " look for statistical regularities in their input data, which turns out to be sufficient to\n",
    " perform well on many simple tasks. In much the same way that computer vision is pattern recognition applied to pixels, NLP is pattern recognition applied to words, sentences, and paragraphs\n",
    "\n",
    "\n",
    "+   Finally, around 2017–2018, a new architecture rose to replace RNNs: the __Transformer__, which you will learn about in the second half of this chapter. Transformers\n",
    " unlocked considerable progress across the field in a short period of time, and today\n",
    " most NLP systems are based on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Preparing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Vectorizing$ text is the process of transforming text into numeric tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $Standardize$:  First, you standardize the text to make it easier to process, such as by converting\n",
    " it to lowercase or removing punctuation.\n",
    "\n",
    "2. $Tokenization$:  You split the text into units (called tokens), such as characters, words, or groups\n",
    " of words. This is called tokenization.  \n",
    " \n",
    "3. $One-Hot-Encode$:  You convert each such token into a numerical vector. This will usually involve\n",
    " first indexing all tokens present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.1 Text standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text standardization is a basic form of feature engineering that aims to __erase\n",
    " encoding differences that you don’t want your model to have to deal with__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  __Convert to lowercase and remove punctuation characters__.\n",
    "    \n",
    "\n",
    "2.  __Stemming__: converting variations of a term (such as different conjugated forms of a verb) into a single shared representation.\n",
    "    “caught” and “been catching” into “[catch]” or “cats” into “[cat]”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.2 Text splitting (tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Word-level tokenization__ —Where tokens are space-separated (or punctuation-separated) substrings. A variant of this is to further split words into subwords\n",
    "when applicable—for instance, treating “staring” as “star+ing” or “called” as\n",
    "“call+ed.”\n",
    "\n",
    "2. __N-gram tokenization__ —Where tokens are groups of N consecutive words. For\n",
    "instance, “the cat” or “he was” would be 2-gram tokens (also called bigrams).\n",
    "\n",
    "\n",
    "3. __Character-level tokenization__ —Where each character is its own token. In practice,\n",
    "this scheme is rarely used, and you only really see it in specialized contexts, like\n",
    "text generation or speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two kinds of text-processing models: \n",
    "1. $Sequence$  $model$ :care about word __order__\n",
    "\n",
    "2. $Bag-of-words$  $model$: treat input words as a set, __discarding their original order__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.3 Vocabulary indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your text is split into tokens, you need to encode each token into a numerical\n",
    " representation.\n",
    "\n",
    "the way you’d go about it is to build\n",
    " an index of all terms found in the training data (the “vocabulary”), and assign a\n",
    " unique integer to each entry in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary = {}\n",
    "# for text in dataset: \n",
    "#     text = standardize(text)\n",
    "#     tokens = tokenize(text)\n",
    "#     for token in tokens:\n",
    "#         if token is not in vocabulary:\n",
    "#             vocabulary[token] = len(vocabulary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then convert that integer into a vector encoding that can be processed by a\n",
    "neural network, like a one-hot vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# def one_hot_encode_token(token):\n",
    "#     vetcor = np.zeros((len(vocabulary),))\n",
    "#     token_index = vocabulary[token]\n",
    "#     vetcor[token_index]\n",
    "#     return vetcor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that at this step it’s common to restrict the vocabulary to only the top 20,000 or\n",
    " 30,000 most common words found in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data you were using from keras.datasets.imdb was\n",
    " already preprocessed into sequences of integers, where each integer stood for a given\n",
    " word. \n",
    " \n",
    "Back then, we used the setting num_words=10000, in order to restrict our vocabulary to the __top 10,000__ most common words found in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we look up a new token in our vocabulary index, it may not necessarily exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your training data may not have contained any instance of the word “cherimoya” (or maybe you\n",
    " excluded it from your index because it was too rare), so doing token_index =\n",
    " vocabulary[\"cherimoya\"] may result in a KeyError. \n",
    " \n",
    "To handle this, you should use\n",
    " an __“out of vocabulary” index__ (abbreviated as $OOV$ index)—a catch-all for any token\n",
    " that wasn’t in the index. \n",
    " \n",
    "It’s usually __index 1__: you’re actually doing token_index =\n",
    " vocabulary.get(token, 1). When decoding a sequence of integers back into words,\n",
    " you’ll replace 1 with something like “[UNK]” (which you’d call an “OOV token”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.4 Using the TextVectorization layer\n",
    "\n",
    "Every step I’ve introduced so far would be very easy to implement in pure Python.\n",
    "Maybe you could write something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self,text:str) -> str:\n",
    "        text = text.lower()\n",
    "        return\"\".join( char for char in text if char not in string.punctuation )\n",
    "\n",
    "    def tokenize(self,text:str)->list:\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "    \n",
    "    def make_vocabulary(self,dataset):\n",
    "        self.vocabulary = {\"\":0,\"[UNK]\":1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens :\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_volcabulary = dict ( (val,key) for key,val in self.vocabulary.items()   )\n",
    "    \n",
    "    def encode(self,text:str) -> list:\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token,1) for token in tokens ]\n",
    "\n",
    "    def decode(self,int_sequence) -> str:\n",
    "        return \"\".join(\n",
    "            self.inverse_volcabulary.get(i,\"[UNK]\") for i in int_sequence\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, '[UNK]': 1, 'i': 2, 'write': 3, 'erase': 4, 'rewrite': 5, 'again': 6, 'and': 7, 'then': 8, 'a': 9, 'poppy': 10, 'blooms': 11}\n"
     ]
    }
   ],
   "source": [
    "vector = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\"\n",
    "]\n",
    "\n",
    "vector.make_vocabulary(dataset)\n",
    "print(vector.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentence:\n",
      "[2, 3, 5, 7, 1, 5, 6]\n",
      "Decoded Sentence\n",
      "iwriterewriteand[UNK]rewriteagain\n"
     ]
    }
   ],
   "source": [
    "test_sequence = \"I write, rewrite, and still rewrite again.\"\n",
    "encoded_sentence = vector.encode(test_sequence)\n",
    "print(f\"Encoded Sentence:\\n{encoded_sentence}\")\n",
    "\n",
    "decoded_sentence = vector.decode(encoded_sentence)\n",
    "print(f\"Decoded Sentence\\n{decoded_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vect = CountVectorizer().fit_transform(vector.vocabulary) \n",
    "# vect = vect.toarray()\n",
    "# print(vect)\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# one = OneHotEncoder().fit_transform(vect)\n",
    "# print(one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, using something like this wouldn’t be very performant. \n",
    "\n",
    "In practice, you’ll work with the Keras __TextVectorization__ layer, which is fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "# Configures the layer to return sequences of words encoded \n",
    "# as integer indices. There are several other output modes \n",
    "# available, which you will see in action in a bit.\n",
    "text_vectorization = TextVectorization(output_mode=\"int\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the TextVectorization layer will use the setting\n",
    "+ __convert to lowercase__ and __remove punctuation__ for text $standardization$, \n",
    "+  __split on whitespace__ for $tokenization$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that\n",
    " such custom functions should operate on __tf.string tensors__, not regular Python\n",
    " strings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor:tf.strings) -> tf.strings:\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\",\"\"\n",
    "    )\n",
    "\n",
    "def custom_split_fn(string_tensor:tf.strings) -> tf.strings:\n",
    "    return tf.strings.split(string_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    output_mode='int',\n",
    "    standardize= custom_standardization_fn,\n",
    "    split= custom_split_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To index the vocabulary of a text corpus, just call the __adapt() method__ of the layer\n",
    "with a Dataset object that yields __strings__, or just with __a list of Python strings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\"\n",
    "]\n",
    "\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can __retrieve the computed vocabulary__ via __get_vocabulary()__—this can\n",
    " be useful if you need to convert text encoded as integer sequences back into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstartion, let's try to encode and  decode the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.1 Displaying the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
      "iwriterewriteand[UNK]rewriteagain\n"
     ]
    }
   ],
   "source": [
    "vocalbuary= text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)\n",
    "\n",
    "inverse_volcab= dict( enumerate(vocalbuary))\n",
    "decoded_sentence = \"\".join( inverse_volcab[int(i)] \n",
    "                            for i in encoded_sentence\n",
    "    )\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the __TextVectorization__ layer in a __tf.data__ pipeline or as part of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  __TextVectorization__ __ONLY__ works on __CPU__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are __Two__ ways to use __TextVectorization__ layer\n",
    "\n",
    "1.  Put it in the __tf.data__ pipeline:\n",
    "    \n",
    "    + __int_sequence_dataset = string_dataset.map( \n",
    "            text_vectorization,\n",
    "            num_parallel_calls=4)__\n",
    "    \n",
    "    +  while the GPU runs the model on one batch of vectorized data, the CPU stays busy by vectorizing the next batch of\n",
    "        raw strings.\n",
    "    \n",
    "    + __Recommands for GPU version__\n",
    "\n",
    "\n",
    "\n",
    "2.  Make it part of the model:\n",
    "    +   __text_input = keras.Input(shape=(), dtype=\"string\")__\n",
    "\n",
    "        __vectorized_text = text_vectorization(text_input)__\n",
    "\n",
    "        __embedded_input = keras.layers.Embedding(...)(vectorized_text)__\n",
    "\n",
    "        __output = ...__\n",
    "\n",
    "        __model = keras.Model(text_input, output)__\n",
    "\n",
    "    +   This means that\n",
    "        at each training step, the rest of the model (placed on the GPU) will have to wait for\n",
    "        the output of the TextVectorization layer (placed on the CPU) to be ready in order\n",
    "        to get to work\n",
    "\n",
    "    +   __Not suitable for GPU version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, the TextVectorization layer enables you to include\n",
    " text preprocessing right into your model, making it easier to deploy—even if you were\n",
    " originally using the layer as part of a tf.data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Two approaches for representing groups of words: Sets and Sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Set : bag-of-words\n",
    "\n",
    "+ Sequence : Focus on __Order__ of sentenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Transformer__ architecture is technically order-agnostic, yet it injects word-position information into\n",
    " the representations it processes, which enables it to simultaneously look at different\n",
    " parts of a sentence (unlike RNNs) while still being order-aware. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because they take into  account word order, both RNNs and Transformers are called $sequence$ $models$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.1 Preparing the IMDB movie reviews data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Take a look at the content of a few of these text files. \n",
    "\n",
    "+ Whether you’re working with\n",
    " text data or image data, remember to always inspect what your data looks like before\n",
    " you dive into modeling it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting apart 20% of the training text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, pathlib,shutil,random\n",
    "\n",
    "# # Find path in the current folder\n",
    "# base_dir  = pathlib.Path(\"E:\\\\Deep Learning with Python\\\\Datas\\\\Ch11_IMBD_RAW\\\\aclImdb_v1\\\\aclImdb\")\n",
    "\n",
    "# val_dir = base_dir / \"val\"\n",
    "# train_dir = base_dir / \"train\"\n",
    "\n",
    "# for category in (\"neg\",\"pos\"):\n",
    "#     os.makedirs(val_dir/category)\n",
    "\n",
    "#     # Make a list of all file name in //pos and //neg\n",
    "#     files  = os.listdir( train_dir / category )\n",
    "#     # Shuffle the list\n",
    "#     random.Random(1337).shuffle(files)\n",
    "\n",
    "#     # Pick last 20% files from the list\n",
    "#     num_val_sample = int(len(files) * 0.2)\n",
    "\n",
    "#     #Create a list for validation data\n",
    "#     val_files = files[-num_val_sample:]\n",
    "\n",
    "#     ## Move them one by one\n",
    "#     for fname in val_files:\n",
    "#         shutil.move(  train_dir/category/fname, val_dir / category / fname   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use __text_dataset_from_directory()__ to create dataset\n",
    "\n",
    "Remember to delete \\\\unsup folder in \\\\train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils import text_dataset_from_directory\n",
    "batch_size = 32\n",
    "train_ds = text_dataset_from_directory(directory=\"E:\\\\Deep Learning with Python\\\\Datas\\\\Ch11_IMBD_RAW\\\\aclImdb_v1\\\\aclImdb\\\\train\",batch_size= batch_size)\n",
    "val_ds = text_dataset_from_directory(directory=\"E:\\\\Deep Learning with Python\\\\Datas\\\\Ch11_IMBD_RAW\\\\aclImdb_v1\\\\aclImdb\\\\val\",batch_size= batch_size)\n",
    "test_ds = text_dataset_from_directory(directory=\"E:\\\\Deep Learning with Python\\\\Datas\\\\Ch11_IMBD_RAW\\\\aclImdb_v1\\\\aclImdb\\\\test\",batch_size= batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.2 Displaying the shapes and dtypes of the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape (32,)\n",
      "Inputs dytpe <dtype: 'string'>\n",
      "Inputs example b'It\\'s good to see that Vintage Film Buff have correctly categorized their excellent DVD release as a \"musical\", for that\\'s what this film is, pure and simple. Like its unofficial remake, Murder at the Windmill (1949), the murder plot is just an excuse for an elaborate girlie show with Kitty Carlisle and Gertrude Michael leading a cast of super-decorative girls including Ann Sheridan, Lucy Ball, Beryl Wallace, Gwenllian Gill, Gladys Young, Barbara Fritchie, Wanda Perry and Dorothy White. Carl Brisson is also on hand to lend his strong voice to \"Cocktails for Two\". Undoubtedly the movie\\'s most popular song, it is heard no less than four times. However, it\\'s Gertrude Michael who steals the show, not only with her rendition of \"Sweet Marijauna\" but her strong performance as the hero\\'s rejected girlfriend. As for the rest of the cast, we could have done without Jack Oakie and Victor McLaglen altogether. The only good thing about Oakie\\'s role is his weak running gag with cult icon, Toby Wing. In fact, to give you an idea as to how far the rest of the comedy is over-indulged and over-strained, super-dumb Inspector McLaglen simply cannot put his hands on the killer even though, would you believe, in this instance it happens to be the person you most suspect. Director Mitch Leisen actually goes to great pains to point the killer out to even the dumbest member of the cinema audience by giving the player concerned close-up after close-up.'\n",
      "Outputs shape (32,)\n",
      "Outputs dytpe <dtype: 'int32'>\n",
      "Outputs example 1\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"Inputs shape {}\".format(inputs.shape))\n",
    "    print(\"Inputs dytpe {}\".format(inputs.dtype))\n",
    "    print(\"Inputs example {}\".format(inputs[0]))\n",
    "\n",
    "    print(\"Outputs shape {}\".format(targets.shape))\n",
    "    print(\"Outputs dytpe {}\".format(targets.dtype))\n",
    "    print(\"Outputs example {}\".format(targets[0]))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.2 Processing words as a set: The bag-of-words approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of this encoding is that you can represent an entire text as a single vector, where each entry is a presence indicator for a given word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s process our raw text datasets with a TextVectorization layer so that\n",
    "they yield multi-hot encoded binary word vectors. \n",
    "\n",
    "Our layer will only look at single\n",
    "words (that is to say, $unigrams$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.3 Preprocessing our datasets with a TextVectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the vocabulary to the 20,000 most frequent words.\n",
    "# Otherwise we’d be indexing every word in the training data—\n",
    "# potentially tens of thousands of terms that only occur once or\n",
    "# twice and thus aren’t informative. In general, 20,000 is the\n",
    "# right vocabulary size for text classification\n",
    "text_vectorization = TextVectorization(max_tokens=20000,output_mode=\"multi_hot\",)\n",
    "\n",
    "# Prepare a dataset that \n",
    "# only yields raw text \n",
    "# inputs (no labels).\n",
    "text_only_train_ds = train_ds.map(lambda x,y:x)\n",
    "\n",
    "# Use that dataset to index \n",
    "# the dataset vocabulary via \n",
    "# the adapt() method.\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_lgram_train_ds = train_ds.map(\n",
    "    lambda x, y : (text_vectorization(x),y), num_parallel_calls = 4\n",
    ")\n",
    "\n",
    "binary_lgram_val_ds = val_ds.map(\n",
    "    lambda x, y : (text_vectorization(x),y), num_parallel_calls = 4\n",
    ")\n",
    "\n",
    "binary_lgram_test_ds = test_ds.map(\n",
    "    lambda x, y : (text_vectorization(x),y), num_parallel_calls = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.4 Inspecting the output of our binary unigram dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape (32, 20000)\n",
      "Inputs dytpe <dtype: 'float32'>\n",
      "Inputs example [1. 1. 1. ... 0. 0. 0.]\n",
      "Outputs shape (32,)\n",
      "Outputs dytpe <dtype: 'int32'>\n",
      "Outputs example 1\n"
     ]
    }
   ],
   "source": [
    "for inputs,targets in binary_lgram_train_ds:\n",
    "    print(\"Inputs shape {}\".format(inputs.shape))\n",
    "    print(\"Inputs dytpe {}\".format(inputs.dtype))\n",
    "    print(\"Inputs example {}\".format(inputs[0]))\n",
    "\n",
    "    print(\"Outputs shape {}\".format(targets.shape))\n",
    "    print(\"Outputs dytpe {}\".format(targets.dtype))\n",
    "    print(\"Outputs example {}\".format(targets[0]))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s write a reusable model-building function that we’ll use in all of our experiments in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.5 Our model-building utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "def get_model(max_tokens= 20000, hidden_dim = 16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim,activation='relu')(inputs)\n",
    "    x  = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs,outputs)\n",
    "    model.compile (\n",
    "        optimizer = keras.optimizers.RMSprop(),\n",
    "        loss = keras.losses.BinaryCrossentropy(),\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.6 Training and testing the binary unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_24 (InputLayer)       [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "callbacks = [ModelCheckpoint(\n",
    "            filepath=\"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\binary_lgrm.keras\",\n",
    "            save_best_only=True\n",
    ")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We call cache() on the \n",
    "# datasets to cache them in \n",
    "# memory: this way, we will \n",
    "# only do the preprocessing \n",
    "# once, during the first \n",
    "# epoch, and we’ll reuse the \n",
    "# preprocessed texts for the \n",
    "# following epochs. This can \n",
    "# only be done if the data \n",
    "# is small enough to fit in \n",
    "# memory\n",
    "# model.fit( binary_lgram_train_ds.cache(), epochs= 10, callbacks=callbacks,validation_data=binary_lgram_val_ds.cache())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\binary_lgrm.keras\")\n",
    "# print(\"Test Acc = {:.3f}\".format(model.evaluate(binary_lgram_test_ds)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BIGRAMS With binary ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.7 Configuring the TextVectorization layer to return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(ngrams=2, max_tokens=20000,output_mode=\"multi_hot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.8 Training and testing the binary bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x,y: (text_vectorization(x),y),num_parallel_calls = 4\n",
    ")\n",
    "\n",
    "\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x,y: (text_vectorization(x),y),num_parallel_calls = 4\n",
    ")\n",
    "\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x,y: (text_vectorization(x),y),num_parallel_calls = 4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "callbacks = [ModelCheckpoint(\n",
    "            filepath=\"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\binary_2grm.keras\",\n",
    "            save_best_only=True\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit( binary_2gram_train_ds.cache(), epochs= 10, callbacks=callbacks,validation_data=binary_2gram_val_ds.cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\binary_2grm.keras\")\n",
    "# print(\"Test Acc = {:.3f}\".format(model.evaluate(binary_2gram_test_ds)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BIGRAMS WITH TF-IDF ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add a bit more information to this representation by __counting how many\n",
    "times each word or N-gram occurs__, that is to say, by taking the histogram of the words\n",
    "over the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.9 Configuring the TextVectorization layer to return token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,max_tokens=20000,output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ To filter those uninformative words, we should \"normalize\" the  wordcounts by \"sparsity\"\n",
    "\n",
    "+ The best practise is go with __TF-IDF__ : $Term$ $Frequency$ -  $Inverse$ $Document$ $Frequency$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is so common that it’s built into the __TextVectorization__ layer. \n",
    "\n",
    "All you need\n",
    " to do to start using it is to switch the __output_mode__ argument to __\"tf_idf\"__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding TF-IDF normalization\n",
    "\n",
    "It weights a given term by taking “term frequency,” how many times the term appears in the\n",
    " current document, and dividing it by a measure of “document frequency,” which estimates how often the term comes up across the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(term,document,dataset):\n",
    "    import math\n",
    "    term_freq= document.count(term)\n",
    "    doc_freq = math.log(  (sum(doc.count(term)) for doc in dataset) +1 )\n",
    "    return term_freq/doc_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.10 Configuring TextVectorization to return TF-IDF-weighted outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens= 20000,\n",
    "    output_mode=\"tf_idf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x,y : (text_vectorization(x),y), num_parallel_calls=4\n",
    ")\n",
    "\n",
    "\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x,y : (text_vectorization(x),y), num_parallel_calls=4\n",
    ")\n",
    "\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x,y : (text_vectorization(x),y), num_parallel_calls=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= get_model()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "callbacks = [ModelCheckpoint(\n",
    "            filepath=\"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\tfidf_2grm.keras\",\n",
    "            save_best_only=True\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit( tfidf_2gram_train_ds.cache(), epochs= 10, callbacks=callbacks,validation_data=tfidf_2gram_val_ds.cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\tfidf_2grm.keras\")\n",
    "# print(\"Test Acc = {:.3f}\".format(model.evaluate(tfidf_2gram_test_ds)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us an 88.5% test accuracy on the IMDB classification task: it doesn’t seem to\n",
    " be particularly helpful in this case. \n",
    " \n",
    " However, for many text-classification datasets, it\n",
    " would be typical to see __a one-percentage-point increase when using TF-IDF__ compared\n",
    " to plain binary encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting a model that processes raw strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create a new model to reuse your trained vectorlization layer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,),dtype=\"string\")\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "\n",
    "## Apply the previously trained model which yields outputs \n",
    "outputs  = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs,outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : b'That was an excellent movie, I love it!'\n",
      "Positive proportion= 48.783 %\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I love it!\"]\n",
    "])\n",
    "\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(\"Input sentence : {}\".format(raw_text_data[0][0]))\n",
    "print(\"Positive proportion= {:.3f} %\".format(float(predictions[0]*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.3 Processing words as a sequence: The sequence model approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Sequence Model$ : Exposed the model to raw word sequences and let it figure out such features on its own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a sequence model, \n",
    "\n",
    "1. Start by representing your input samples as\n",
    " __sequences of integer indices__ (one integer standing for one word). \n",
    " \n",
    "2. Then, you’d map each integer to a vector to obtain vector sequences. \n",
    "\n",
    "3. Finally, you’d feed these\n",
    " sequences of vectors into a stack of layers that could cross-correlate features from adjacent vectors, such as a 1D convnet, a RNN, or a Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A residual stack of depth wise-separable 1D convolutions__ can often achieve comparable performance to __a bidirectional LSTM__, at a greatly reduced computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Practical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.12 Preparing integer sequence datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x,y : (text_vectorization(x),y), num_parallel_calls =4\n",
    ")\n",
    "\n",
    "\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x,y : (text_vectorization(x),y), num_parallel_calls =4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x,y : (text_vectorization(x),y), num_parallel_calls =4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.13 A sequence model built on one-hot encoded vector sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The simplest way to convert our integer sequences to vector\n",
    " sequences is to one-hot encode the integers (each dimension would represent one\n",
    " possible term in the vocabulary). \n",
    " \n",
    "+ On top of these one-hot vectors, we’ll add a simple\n",
    " bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_28 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " tf.one_hot_2 (TFOpLambda)   (None, None, 20000)       0         \n",
      "                                                                 \n",
      " bidirectional_13 (Bidirecti  (None, 64)               5128448   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,128,513\n",
      "Trainable params: 5,128,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,),dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs,depth=max_tokens)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs,outputs)\n",
    "model.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = keras.losses.BinaryCrossentropy(),\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.14 Training a first basic sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "callbacks = [ModelCheckpoint(\n",
    "            filepath=\"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\one_hot_bidir_lstm.keras\",\n",
    "            save_best_only=True\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit( int_train_ds, epochs= 10, callbacks=callbacks,validation_data=int_val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\one_hot_bidir_lstm.keras\")\n",
    "# print(\"Test Acc = {:.3f}\".format(model.evaluate(tfidf_2gram_test_ds)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results will not give any improvement thus we do not need to run them. \n",
    "\n",
    "Let's focus on the embedded method !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $One-hot-encode$ : \n",
    "    1. Sparse\n",
    "    2. Assume all the tokens are independent to each other (which is not true for words)\n",
    "    3. Usually in very high dimension\n",
    "\n",
    "+ $Embedding$:\n",
    "    1. Map human language into a __geometry structure__\n",
    "    2. Similar word get similar __locations__ and __directions__\n",
    "    3. Usually in low dimension\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways to obtain word embeddings:\n",
    "\n",
    "1. Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with ran\u0002 dom word vectors and then learn word vectors in the same way you learn the\n",
    " weights of a neural network.\n",
    "\n",
    "2. Load into your model word embeddings that were precomputed using a different machine learning task than the one you’re trying to solve. These are called\n",
    " pretrained word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LEARNING WORD EMBEDDINGS WITH THE EMBEDDING LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.15 Instantiating an Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(input_dim= max_tokens,output_dim=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ It takes __integers__ as input, looks up\n",
    " these integers in an internal dictionary, and returns the associated vectors.\n",
    "\n",
    "+ The Embedding layer takes as input a __rank-2 tensor of integers__, of shape __(batch_size, sequence_length)__,\n",
    "\n",
    "+ The layer then returns\n",
    " a 3D floating-point tensor of shape __(batch_size, sequence_length, embedding_dimensionality)__\n",
    "\n",
    "+ When you instantiate an Embedding layer, its weights (its internal dictionary o \n",
    " token vectors) are initially random, just as with any other layer.\n",
    "\n",
    "+ Once fully trained, the embedding\n",
    " space will show a lot of structure—a kind of structure specialized for the specific prob\u0002 lem for which you’re training your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.16 Model that uses an Embedding layer trained from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,),dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens,output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1,activation='sigmoid')(x)\n",
    "model = keras.Model(inputs,outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [ModelCheckpoint(\n",
    "        filepath=\"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\embeddings_bidir_gru.keras\",\n",
    "        save_best_only=True\n",
    ")]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_29 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_18 (Embedding)    (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional_14 (Bidirecti  (None, 64)               73984     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,194,049\n",
      "Trainable params: 5,194,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(int_train_ds,validation_data=int_val_ds,epochs=10,callbacks=callbacks,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\embeddings_bidir_gru.keras\")\n",
    "# print(\"Test Acc = {:.3f}\".format(model.evaluate(int_test_ds)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UNDERSTANDING PADDING AND MASKING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The main problem of previous example:\n",
    "\n",
    "    This comes from our use of the output_sequence_length=max_\n",
    "    length option in TextVectorization (with max_length equal to 600): sentences longer than 600 tokens are truncated to a length of 600 tokens, and sentences shorter\n",
    "    than 600 tokens are padded with zeros at the end so that they can be concatenated\n",
    "    together with other sequences to form contiguous batches.\n",
    "\n",
    "+ The RNN that looks at the tokens in their natural order will spend\n",
    " its last iterations seeing only vectors that encode padding—possibly for several hundreds of iterations if the original sentence was short. The information stored in the\n",
    " internal state of the RNN will gradually fade out as it gets exposed to these meaningless inputs.\n",
    "\n",
    "+ We need $masking$ to tell RNN where should stop the iteration.\n",
    "\n",
    "+ You can retrive that by passing __mask_zero=True__ in the __Embedded()__ layer\n",
    "\n",
    "+ By calling __.compute_mask()__ to retrive the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True  True False False False]\n",
      " [ True  True  True  True  True False False]\n",
      " [ True  True False False False False False]], shape=(3, 7), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer =layers.Embedding(input_dim=10, output_dim=256, mask_zero=True)\n",
    "some_input = [\n",
    " [4, 3, 2, 1, 0, 0, 0],\n",
    " [5, 4, 3, 2, 1, 0, 0],\n",
    " [2, 1, 0, 0, 0, 0, 0]]\n",
    "mask = embedding_layer.compute_mask(some_input)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.17 Using an Embedding layer with masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_30 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_20 (Embedding)    (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional_15 (Bidirecti  (None, 64)               73984     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,194,049\n",
      "Trainable params: 5,194,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(\n",
    " input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    " loss=\"binary_crossentropy\",\n",
    " metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Sometimes your dataset is too small for generating embeddings\n",
    "\n",
    "+ There are various precomputed databases of word embeddings that you can download and use in a Keras __Embedding__ layer\n",
    "\n",
    "+ Popular : $Glove$ and $Word2Vec$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s download the GloVe word embeddings precomputed on the 2014\n",
    "English Wikipedia dataset.\n",
    "\n",
    "Let’s parse the unzipped file (a .txt file) to build an index that maps words (as strings)\n",
    "to their vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.18 Parsing the GloVe word-embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "path_to_glove_file = \"E:\\\\Deep Learning with Python\\\\Datas\\\\Ch11_IMBD_RAW\\\\glove.6B\\\\glove.6B.100d.txt\"\n",
    "\n",
    "embedding_index = {}\n",
    "\n",
    "with open(path_to_glove_file,encoding='utf-8') as f:\n",
    "    read = f.readlines()\n",
    "    for line in read:\n",
    "        word,coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs,\"f\",sep=\" \")\n",
    "        embedding_index[word] = coefs\n",
    "f.close()\n",
    "print(\"Found {} word vectors\".format(len(embedding_index)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s build an embedding matrix that you can load into an Embedding layer. It\n",
    "must be a matrix of shape (max_words, embedding_dim), where each entry i contains\n",
    "the embedding_dim-dimensional vector for the word of index i in the reference word\n",
    "index (built during tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.19 Preparing the GloVe word-embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "# Retrieve the vocabulary indexed by\n",
    "# our previous TextVectorization layer.\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "\n",
    "# Use it to create a \n",
    "# mapping from words \n",
    "# to their index in the \n",
    "# vocabulary\n",
    "word_index =dict(zip(vocabulary,range(len(vocabulary))))\n",
    "\n",
    "# Prepare a matrix \n",
    "# that we’ll fill with \n",
    "# the GloVe vectors.\n",
    "embedding_matrix = np.zeros((max_tokens,embedding_dim))\n",
    "\n",
    "\n",
    "\n",
    "# Fill entry i in the matrix with the \n",
    "# word vector for index i. Words \n",
    "# not found in the embedding \n",
    "# index will be all zeros.\n",
    "for word, i in word_index.items():\n",
    "    if i<max_tokens:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use a Constant initializer to load the pretrained embeddings in an Embedding\n",
    " layer. So as not to disrupt the pretrained representations during training, we __freeze the layer via trainable=False:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    max_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer= keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    mask_zero=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.20 Model that uses a pretrained Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_31 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_21 (Embedding)    (None, None, 100)         2000000   \n",
      "                                                                 \n",
      " bidirectional_16 (Bidirecti  (None, 64)               34048     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,034,113\n",
      "Trainable params: 34,113\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = embedding_layer(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    " loss=\"binary_crossentropy\",\n",
    " metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "callbacks = [\n",
    " keras.callbacks.ModelCheckpoint(\"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\glove_embeddings_sequence_model.keras\",\n",
    " save_best_only=True) ]\n",
    "# model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
    "#  callbacks=callbacks)\n",
    "# model = keras.models.load_model(\"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\glove_embeddings_sequence_model.keras\")\n",
    "# print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 The Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Transformer is overtaking the RNN these years.\n",
    "\n",
    "+ The Machanism behind Transformer : $Neural$ $Attention$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.1 Understanding self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Simple bu Powerful idea : Pay attention to some features , not all of them.\n",
    "\n",
    "+ Some similar concepts: \n",
    "    1. Maxpooling\n",
    "    2. TF-IDF normalization : A continuous of attention.\n",
    "\n",
    "+ It can be made for $text-aware$ since the same word in different sentence has different meaning.\n",
    "\n",
    "+ $self-attention$: To modulate the representation of tokens by using the representation of related tokens in sequences.\n",
    "\n",
    "+ Step of $self-attention$: \n",
    "    1. Compute relevancy scores between the vector for “station” and every other word in the sentence.\n",
    "    2. Compute the sum of all word vectors in the sentence, weighted by our relevancy scores\n",
    "    3. The resulting vector is our new representation of the specific word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the pesudocode of self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(input_sequence:np.array):\n",
    "    output = np.zeros(shape=input_sequence.shape)\n",
    "\n",
    "    # Iterate over each token in the input sequence.\n",
    "    for i, pivot_vector in enumerate(input_sequence):\n",
    "        \n",
    "        scores = np.zeros(shape=(len(input_sequence)))\n",
    "        \n",
    "        #  Compute the dot  product (attention  score) between the token and every  other token\n",
    "        for j, vector in enumerate(input_sequence):\n",
    "            scores[j] = np.dot(pivot_vector,vector.T )\n",
    "\n",
    "        # Scale by a normalization factor, and apply a softmax.    \n",
    "        scores /= np.sqrt(input_sequence.shape[1])\n",
    "        scores = softmax(scores)\n",
    "\n",
    "\n",
    "        new_pivot_representation = np.zeros(shape=(pivot_vector.shape))\n",
    "\n",
    "        for j, vector in enumerate(input_sequence):\n",
    "\n",
    "            # Take the sum of all tokens weighted by the attention scores.\n",
    "            new_pivot_representation  += vector * scores[j]\n",
    "        \n",
    "        output[i] = new_pivot_representation\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practise it looks like following: \n",
    "\n",
    "1. Why are we passing the inputs to the layer three times? That seems redundant.\n",
    "\n",
    "2. What are these “multiple heads” we’re referring to? That sounds intimidating—\n",
    "do they also grow back if you cut them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_heads = 4\n",
    "# embed_dim = 256\n",
    "# inputs = keras.Input(shape=(None,))\n",
    "# mha_layer = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "# outputs = mha_layer(inputs, inputs, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GENERALIZED SELF-ATTENTION: THE QUERY-KEY-VALUE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ A Transformer is a $sequence-to-sequence$ model: it was designed to\n",
    " convert one sequence into another\n",
    "\n",
    "+ The meaning of $self-attention$: This means “for each token in inputs (A), compute how much the token is related to\n",
    " every token in inputs (B), and use these scores to weight a sum of tokens from\n",
    " inputs (C).”\n",
    " \n",
    "    __outputs = sum( inputsC * pairwise_scores( inputsA,inputsB ))__\n",
    "\n",
    "+  Inputs A :  $query$ : Something you are looking for\n",
    "   \n",
    "   Inputs B :  $key$ : Assigned to each value that describes the value in a format that can be readily compared to a $query$.\n",
    "\n",
    "   Inputs C : $values$ : A body of knowledge that you are trying to extract information from\n",
    "\n",
    "\n",
    "   __outputs = sum( values * pairwise_scores( query,key ))__\n",
    "\n",
    "+ In practice, the $keys$ and the $values$ are often the __same sequence__\n",
    "\n",
    "+ That explains why we needed to pass inputs three times to our MultiHeadAttention\n",
    "layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.2 Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $Multi-head$:   Output space of the self-attention layer gets factored into a set of independent subspaces, learned separately.\n",
    "    \n",
    "    1. The initial query, key, and value are sent through __three independent sets of dense projections__, resulting in __three separate vectors__.\n",
    "\n",
    "    2. Each vector is processed via neural attention, and the __three outputs__ are __concatenated__ back together into a single output sequence.\n",
    "\n",
    "    3. Each dense project is called $head$\n",
    "\n",
    "+ Advantages : \n",
    "\n",
    "    1. The learnabel Dense projections make sure the layer actually learn something.\n",
    "\n",
    "    2. Helps layer learn different features of tokens\n",
    "\n",
    "+ Similar to the $Depthwise$ $Separable$ $Convolutions$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.3 The Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Factoring outputs into multiple independent spaces \n",
    "+ adding residual connections\n",
    "+ adding normalization layers\n",
    "\n",
    "+ The original Transformer : \n",
    "    1. A $Transformer$ $encoder$ that processes the source sequence \n",
    "    2. A $Transformer$ $decoder$ that uses the source sequence to generate a translated version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.21 Transformer encoder implemented as a subclassed Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self,embed_dim,dense_dim,num_heads,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,key_dim=embed_dim\n",
    "        )\n",
    "        self.dens_proj = keras.Sequential([\n",
    "            layers.Dense(dense_dim,activation='relu'),\n",
    "            layers.Dense(embed_dim,)\n",
    "        ])\n",
    "        self.layernorm_1 = layers.LayerNormalization() \n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self,inputs,mask=None):\n",
    "\n",
    "        # The mask that will be generated by \n",
    "        # the Embedding layer will be 2D, but \n",
    "        # the attention layer expects to be 3D \n",
    "        # or 4D, so we expand its rank.\n",
    "        if mask is not None:\n",
    "            mask= mask[:,tf.newaxis,:]\n",
    "        \n",
    "        attention_output = self.attention(inputs,inputs,attention_mask = mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dens_proj(proj_input)\n",
    "\n",
    "        return self.layernorm_2(proj_input+proj_output)\n",
    "        \n",
    "    # Implement \n",
    "    # serialization so \n",
    "    # we can save the \n",
    "    # model.\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\":self.embed_dim,\n",
    "            \"num_heads\" : self.num_heads,\n",
    "            \"dense_dim\" : self.dense_dim\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = layers.PositionalEmbedding(sequence_length, input_dim, output_dim)\n",
    "# config = layer.get_config()\n",
    "#  new_layer = PositionalEmbedding.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are loading a model from checkpoint file, you should provide custom layer classes to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\n",
    "#  filename, custom_objects={\"PositionalEmbedding\": PositionalEmbedding})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that in the __TransformEncoder__ we use __layernormalization__ rather __batchnormalization__. \n",
    "\n",
    "+ __BatchNormalization__ collects information from many samples to obtain accurate statistics for the feature means and variances, \n",
    "\n",
    "+ __LayerNormalization__ pools data within each sequence separately, which is more appropriate for sequence data.\n",
    "\n",
    "Let's check the difference between them by pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_normalization(batch_of_sequences):\n",
    "    mean = np.mean( batch_of_sequences,keepdims=True,axis=-1 )\n",
    "    variance  = np.var(batch_of_sequences,keepdims=True,axis=-1)\n",
    "    return (batch_of_sequences-mean) / variance\n",
    "\n",
    "def batch_normalization(batch_of_images):\n",
    "    mean = np.mean( batch_of_images,keepdims=True,axis=(0,1,2) )\n",
    "    variance  = np.var(batch_of_images,keepdims=True,axis=(0,1,2))\n",
    "    return (batch_of_images-mean) / variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.22 Using the Transformer encoder for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_33 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_23 (Embedding)    (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " transformer_encoder_2 (Tran  (None, None, 256)        543776    \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,664,033\n",
      "Trainable params: 5,664,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,),dtype=\"int64\")\n",
    "x =layers.Embedding(vocab_size,embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim=embed_dim,dense_dim=dense_dim,num_heads=num_heads)(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs,outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.23 Training and evaluating the Transformer encoder based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 48s 74ms/step - loss: 0.4845 - accuracy: 0.7799 - val_loss: 0.3406 - val_accuracy: 0.8602\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 47s 74ms/step - loss: 0.3095 - accuracy: 0.8701 - val_loss: 0.2962 - val_accuracy: 0.8738\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.2354 - accuracy: 0.9068 - val_loss: 0.2923 - val_accuracy: 0.8770\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.1835 - accuracy: 0.9295 - val_loss: 0.3269 - val_accuracy: 0.8714\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.1509 - accuracy: 0.9433 - val_loss: 0.3000 - val_accuracy: 0.8884\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.1293 - accuracy: 0.9525 - val_loss: 0.3716 - val_accuracy: 0.8748\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.1079 - accuracy: 0.9614 - val_loss: 0.3712 - val_accuracy: 0.8790\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.0939 - accuracy: 0.9664 - val_loss: 0.8720 - val_accuracy: 0.7978\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.0801 - accuracy: 0.9710 - val_loss: 0.5679 - val_accuracy: 0.8632\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 47s 76ms/step - loss: 0.0652 - accuracy: 0.9769 - val_loss: 0.6303 - val_accuracy: 0.8600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22d2f279250>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# callbacks = [\n",
    "#  keras.callbacks.ModelCheckpoint(\"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\transformer_encoder.keras\",\n",
    "#  save_best_only=True) ]\n",
    " \n",
    "# model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
    "#  callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 20s 25ms/step - loss: 0.3242 - accuracy: 0.8645\n",
      "Test acc: 0.864\n"
     ]
    }
   ],
   "source": [
    "# model = keras.models.load_model(\n",
    "#  \"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\transformer_encoder.keras\",\n",
    "#  custom_objects={\"TransformerEncoder\": TransformerEncoder}) \n",
    "\n",
    "# print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is even worse , that is because what we gave was __NOT a Sequence__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mentioned in passing that Transformer was a hybrid approach that is technically order-agnostic, but that manually injects order information in the representations it processes. This is the missing ingredient! It’s called positional encoding. Let’s take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USING POSITIONAL ENCODING TO RE-INJECT ORDER INFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Idea :  Give the model access to __WordOorder information__\n",
    "\n",
    "+ Our word embedding will contain two vetcors:\n",
    "    1. The usual word vector\n",
    "    2. The position vector\n",
    "\n",
    "+ The position vector can have a very large scale.\n",
    "\n",
    "+  we’ll learn position embedding vectors the same way we learn to embed word indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.24 Implementing positional embedding as a subclassed layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self,sequence_length,input_dim,output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Embedding for words\n",
    "        self.token_embeddings = layers.Embedding(input_dim=input_dim,output_dim=output_dim)\n",
    "\n",
    "        #Embedding for positions\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length,output_dim=output_dim)\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0,limit=length,delta=1)\n",
    "        embedded_tokens =self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(inputs)\n",
    "        return embedded_tokens+embedded_positions\n",
    "    \n",
    "    def compute_mask(self,inputs,mask=None):\n",
    "        return tf.math.not_equal(inputs,0)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "                \"output_dim\":self.output_dim,\n",
    "                \"input_dim\":self.input_dim,\n",
    "                \"sequence_length\":self.sequence_length\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating addition of Embedding layers\n",
    "# inputs = keras.Input(shape=(None,))\n",
    "# x1 = layers.Embedding(3,3)(inputs)\n",
    "# x2= layers.Embedding(3,3)(inputs)\n",
    "# x3 = x1+x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting all of them togather!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_36 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding_1 (Pos  (None, None, 256)        5273600   \n",
      " itionalEmbedding)                                               \n",
      "                                                                 \n",
      " transformer_encoder_4 (Tran  (None, None, 256)        543776    \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " global_max_pooling1d_4 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,817,633\n",
      "Trainable params: 5,817,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,),dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length,vocab_size,embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim,dense_dim,num_heads)(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs,outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics =['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 52s 78ms/step - loss: 0.4352 - accuracy: 0.8112 - val_loss: 0.2874 - val_accuracy: 0.8784\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 49s 78ms/step - loss: 0.2381 - accuracy: 0.9061 - val_loss: 0.2714 - val_accuracy: 0.8938\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 49s 79ms/step - loss: 0.1924 - accuracy: 0.9282 - val_loss: 0.3545 - val_accuracy: 0.8950\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 49s 79ms/step - loss: 0.1701 - accuracy: 0.9359 - val_loss: 0.2733 - val_accuracy: 0.8972\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 49s 79ms/step - loss: 0.1496 - accuracy: 0.9454 - val_loss: 0.3025 - val_accuracy: 0.8926\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 49s 78ms/step - loss: 0.1390 - accuracy: 0.9491 - val_loss: 0.3365 - val_accuracy: 0.8878\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 49s 79ms/step - loss: 0.1279 - accuracy: 0.9545 - val_loss: 0.3652 - val_accuracy: 0.8900\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 49s 78ms/step - loss: 0.1198 - accuracy: 0.9578 - val_loss: 0.3663 - val_accuracy: 0.8904\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 49s 78ms/step - loss: 0.1162 - accuracy: 0.9591 - val_loss: 0.3250 - val_accuracy: 0.8930\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 49s 78ms/step - loss: 0.1074 - accuracy: 0.9625 - val_loss: 0.3750 - val_accuracy: 0.8856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22cf4147c40>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# callbacks = [\n",
    "#  keras.callbacks.ModelCheckpoint(\"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\full_transformer_encoder.keras\",\n",
    "#  save_best_only=True)\n",
    "# ] \n",
    "# model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, \n",
    "# callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 21s 27ms/step - loss: 0.2849 - accuracy: 0.8844\n",
      "Test acc: 0.884\n"
     ]
    }
   ],
   "source": [
    "# model = keras.models.load_model(\n",
    "#  \"E:\\\\Python-Machine-Learning\\\\Deep_Learning_With_python\\\\Ch11_Imdb\\\\full_transformer_encoder.keras\",\n",
    "#  custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "#  \"PositionalEmbedding\": PositionalEmbedding}) \n",
    "# print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.4 When to use sequence models over bag-of-words models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ratio$ =${Number-of-Samples}\\over{Mean-Sample-Length}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ When $ratio$ > 1500 : __Sequence Model__\n",
    "\n",
    "+ When $ratio$ < 1500 : __Bag_of_bigrams__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 Beyond text classification: Sequence-to-sequence learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ A general model of sequnence-to-sequence model :\n",
    "    + An $encoder$ model turns the source sequence into an intermediate representation.\n",
    "    \n",
    "    + A $decoder$ is trained to predict the next token i in the target sequence by looking at both previous tokens (0 to i - 1) and the encoded source sequence.\n",
    "\n",
    "+ During inference, we don’t have access to the target sequence—we’re trying to predict it from scratch. We’ll have to generate it one token at a time:\n",
    "    1. We obtain the encoded source sequence from the encoder.\n",
    "\n",
    "    2. The decoder starts by looking at the encoded source sequence as well as an initial “seed” token (such as the string \"[start]\"), and uses them to predict the\n",
    "first real token in the sequence\n",
    "\n",
    "    3. The predicted sequence so far is fed back into the decoder, which generates the\n",
    " next token, and so on, until it generates a stop token (such as the string\n",
    " \"[end]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.1 A machine translation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"E:\\\\Deep Learning with Python\\Datas\\\\Ch11_IMBD_RAW\\\\spa-eng\\\\spa-eng\\\\spa.txt\"\n",
    "with open(text_file,encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    english,spanish = line.split(\"\\t\")\n",
    "    spanish = \"[start]\" + spanish + \"[end]\"\n",
    "    text_pairs.append((english,spanish))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('On my way here, the strong wind blew my umbrella inside out.', '[start]De camino aquí una fuerte ráfaga de aire me dio la vuelta al paraguas.[end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15*len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2*num_val_samples\n",
    "\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples+num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples+num_val_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s prepare two separate TextVectorization layers: one for English and one\n",
    " for Spanish:\n",
    "\n",
    "1. We need to preserve the \"[start]\" and \"[end]\" tokens that we’ve inserted. By\n",
    " default, the characters [ and ] would be stripped, but we want to keep them\n",
    " around so we can tell apart the word “start” and the start token \"[start]\".  \n",
    "\n",
    "2. Punctuation is different from language to language! In the Spanish Text Vectorization layer, if we’re going to strip punctuation characters, we need to also strip the character ¿."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.26 Vectorizing the English and Spanish text pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import re\n",
    "strip_chars = string.punctuation+ \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\") \n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string): \n",
    "    lowercase = tf.strings.lower(input_string) \n",
    "    \n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000 \n",
    "sequence_length = 20 \n",
    "#English Layer\n",
    "source_vectorization = layers.TextVectorization( \n",
    " max_tokens=vocab_size,\n",
    " output_mode=\"int\",\n",
    " output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Spanish Layer\n",
    "target_vectorization = layers.TextVectorization( \n",
    " max_tokens=vocab_size,\n",
    " output_mode=\"int\",\n",
    "#  Generate Spanish sentences \n",
    "# that have one extra token, \n",
    "# since we’ll need to offset \n",
    "# the sentence by one step \n",
    "# during training.\n",
    " output_sequence_length=sequence_length + 1, \n",
    " standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts) \n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.27 Preparing datasets for the translation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng,spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return({\n",
    "        \"english\":eng,\n",
    "        \"spanish\":spa[:,:-1]\n",
    "            } ,\n",
    "            spa[:,1:]\n",
    "            )\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts,spa_texts = zip(*pairs)\n",
    "    eng_texts  = list(eng_texts)\n",
    "    spa_texts  = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts,spa_texts))\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    dataset = dataset.map(format_dataset,num_parallel_calls=4)\n",
    "\n",
    "    return dataset.shuffle(2049).prefetch(16).cache()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.2 Sequence-to-sequence learning with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The simplest way to use RNN:\n",
    "\n",
    "\n",
    "    inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
    "\n",
    "    x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n",
    "\n",
    "    x = layers.LSTM(32, return_sequences=True)(x)\n",
    "\n",
    "    outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    __Probelm__:\n",
    "    \n",
    "        1. The target sequence must always be the same length as the source sequence.\n",
    "        \n",
    "        2. Due to the step-by-step nature of RNNs, the model will only be looking at\n",
    "        tokens 0…N in the source sequence in order to predict token N in the target\n",
    "        sequence.\n",
    "\n",
    "+ A proper way to use RNN:\n",
    "\n",
    "    1. Use an RNN (the encoder) to turn the entire source sequence into a single vector (or set of vectors).\n",
    "\n",
    "    2. Then you would use this vector (or vectors) as the $initial$ $state$ of another RNN (the decoder), which would look at elements 0…N in the target sequence, and\n",
    "    try to predict step N+1 in the target sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.28 GRU-based encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "#Specific the Name\n",
    "source = keras.Input(shape=(None,),dtype=\"int64\",name=\"english\")\n",
    "## Do Not forget the mask here !!!!\n",
    "x = layers.Embedding(vocab_size,embed_dim,mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(\n",
    "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.29 GRU-based decoder and the end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Spanish target goes here\n",
    "\n",
    "past_target = keras.Input(shape=(None,),dtype=\"int64\",name=\"spanish\")\n",
    "x = layers.Embedding(vocab_size,embed_dim,mask_zero=True)(past_target)\n",
    "decoder_gru = layers.GRU(latent_dim,return_sequences=True)\n",
    "x = decoder_gru(x,initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "targets_next_step = layers.Dense(vocab_size,activation='softmax')(x)\n",
    "seq2seq_rnn = keras.Model([source,past_target],targets_next_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.30 Training our recurrent sequence-to-sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 144s 102ms/step - loss: 1.3331 - accuracy: 0.3915 - val_loss: 1.1373 - val_accuracy: 0.4618\n"
     ]
    }
   ],
   "source": [
    "seq2seq_rnn.compile(\n",
    " optimizer=\"rmsprop\",\n",
    " loss=\"sparse_categorical_crossentropy\",\n",
    " metrics=['accuracy'])\n",
    "history= seq2seq_rnn.fit(train_ds, epochs=1, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practise, accuracy is not a greate metric for machine translation models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 11.31 Translating new sentences with our RNN encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "spa_vocab = target_vectorization.get_vocabulary() \n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab)) \n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\" \n",
    "\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "\n",
    "        ## Sample the next token\n",
    "        next_token_predictions = seq2seq_rnn.predict( \n",
    "        [tokenized_input_sentence, tokenized_target_sentence]) \n",
    "\n",
    "\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "\n",
    "        ## Convert next prediction token prediction to a string \n",
    "        ## and append it to the generated sentence\n",
    "        sampled_token = spa_index_lookup[sampled_token_index] \n",
    "        \n",
    "        decoded_sentence += \" \" + sampled_token \n",
    "        \n",
    "        ## Exit Condition\n",
    "        if sampled_token == \"[end]\": \n",
    "            break\n",
    "    \n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_eng_texts = [pair[0] for pair in test_pairs] \n",
    "# for _ in range(2):\n",
    "#  input_sentence = random.choice(test_eng_texts)\n",
    "#  print(\"-\")\n",
    "#  print(input_sentence)\n",
    "#  print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawbacks of RNN model:\n",
    "\n",
    "\n",
    "1. The source sequence representation has to be held entirely in the encoder state vector(s)\n",
    "\n",
    "2. RNNs have trouble dealing with very long sequences, since they tend to progressively forget about the past—by the time you’ve reached the 100th token in\n",
    " either sequence, little information remains about the start of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e450050b432e843bda3c41bf3272c133bfc370a7003f3e377e27f87a49ce1127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
