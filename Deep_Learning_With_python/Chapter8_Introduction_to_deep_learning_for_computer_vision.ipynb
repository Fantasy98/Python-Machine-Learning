{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 Introduction to Deep Learning for Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter introduces convolutional neural networks, also known as convnets, the\n",
    " type of deep learning model that is now used almost universally in computer vision\n",
    " applications. You’ll learn to apply convnets to image-classification problems—in particular those involving small training datasets, which are the most common use case i \n",
    " you aren’t a large tech company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Introduction to convnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s take a practical look at a simple convnet example that classifies MNIST digits, a task we performed in chapter 2 using a\n",
    "densely connected network (our test accuracy then was 97.8%). Even though the\n",
    "convnet will be basic, its accuracy will blow our densely connected model from chapter 2 out of the water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following listing shows what a basic convnet looks like. \n",
    "\n",
    "It’s a stack of Conv2D and MaxPooling2D layers. \n",
    "\n",
    "You’ll see in a minute exactly what they do. We’ll build the\n",
    "model using the Functional API, which we introduced in the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(28,28,1))\n",
    "\n",
    "x = layers.Conv2D(filters=32,kernel_size=3,activation='relu')(inputs)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64,kernel_size=3,activation='relu')(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128,kernel_size=3,activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(10,activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs = inputs,outputs=outputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, a convnet takes as input tensors of shape __(image_height, image_width,\n",
    "image_channels)__, not including the batch dimension. \n",
    "\n",
    "In this case, we’ll configure the\n",
    "convnet to process inputs of size __(28, 28, 1)__, which is the format of MNIST images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 8.2 Displaying the model’s summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 13, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,202\n",
      "Trainable params: 104,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You can see that the output of every Conv2D and MaxPooling2D layer is a rank-3 tensor\n",
    "of shape (height, width, channels). The width and height dimensions tend to\n",
    "shrink as you go deeper in the model. The number of channels is controlled by the\n",
    "first argument passed to the Conv2D layers (32, 64, or 128).\n",
    "\n",
    "\n",
    "2. After the last Conv2D layer, we end up with an output of shape (3, 3, 128)—a 3 × 3\n",
    "feature map of 128 channels. The next step is to feed this output into a densely connected classifier like those you’re already familiar with: a stack of Dense layers. These\n",
    "classifiers process vectors, which are 1D, whereas the current output is a rank-3 tensor.\n",
    "\n",
    "\n",
    "3. To bridge the gap, we flatten the 3D outputs to 1D with a Flatten layer before adding\n",
    "the Dense layers.\n",
    " \n",
    " \n",
    "4. Finally, we do 10-way classification, so our last layer has 10 outputs and a softmax\n",
    "activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s train the convnet on the MNIST digits. We’ll reuse a lot of the code from\n",
    " the MNIST example in chapter 2. \n",
    " \n",
    "Because we’re doing 10-way classification with a\n",
    " softmax output, we’ll use the categorical crossentropy loss, and because our labels are\n",
    " integers, we’ll use the sparse version, sparse_categorical_crossentropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 8.3 Training the convnet on MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(train_images,train_labels),(test_images,test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000,28,28,1))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape((10000,28,28,1))\n",
    "test_images = test_images.astype('float32')/255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 13s 8ms/step - loss: 0.1518 - accuracy: 0.9530\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 7s 7ms/step - loss: 0.0431 - accuracy: 0.9866\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.0296 - accuracy: 0.9910\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0226 - accuracy: 0.9930\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0173 - accuracy: 0.9949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17382379370>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer = keras.optimizers.RMSprop(),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(train_images,train_labels,epochs=5,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the evaluation accuacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0258 - accuracy: 0.9916\n",
      "Test Accuracy: 0.991599977016449\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,test_labels)\n",
    "print(\"Test Accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 The convolution operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Foundamental difference between a __densely connected layer__ and a __convolution layer__:\n",
    "\n",
    "__Dense layers__ learn __global patterns__ in their input feature space (for exam ple, for a MNIST digit, patterns involving all pixels), \n",
    "\n",
    "whereas __convolution layers__ learn\n",
    " __local patterns__—in the case of images, patterns found in small 2D windows of the\n",
    " inputs, in previous example, this window has size of 3x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Key characteristic gives convnets TWO interesting properties:\n",
    "\n",
    "1. The patterns they learn are ${translation-invariant}$. \n",
    "\n",
    "After learning a certain pattern in\n",
    "the lower-right corner of a picture, a convnet can recognize it anywhere: for\n",
    "example, in the upper-left corner. A densely connected model would have to\n",
    "learn the pattern anew if it appeared at a new location. \n",
    "\n",
    "This makes convnets data-efficient when processing images (because the visual world is fundamentally\n",
    "translation-invariant): they need fewer training samples to learn representations\n",
    "that have generalization power.\n",
    "\n",
    "2. They can learn ${spatial}$ ${hierarchies}$ ${of}$ ${patterns}$. \n",
    "\n",
    "A first convolution layer will learn\n",
    "small local patterns such as edges, a second convolution layer will learn larger\n",
    "patterns made of the features of the first layers, and so on\n",
    "\n",
    "This allows convnets to efficiently learn increasingly complex and abstract visual concepts, because the visual world is fundamentally spatially hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutions operate over rank-3 tensors called ${feature}$ ${maps}$: \n",
    "\n",
    "With two $spatial$ $axes$\n",
    "(height and width) as well as a $depth$ $axis$ (also called the channels axis). \n",
    "\n",
    "+ For an RGB image, the dimension of the depth axis is 3, because the image has three color channels: red, green, and blue. \n",
    "\n",
    "+ For a black-and-white picture, like the MNIST digits, the\n",
    "depth is 1 (levels of gray)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution operation extracts patches from its $input$\n",
    "$feature$ $map$ and applies the same transformation to all of these patches, producing\n",
    "an $output$ $feature$ $map$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$output$ $feature$ $map$:\n",
    "\n",
    "+ Has  $spatial$ $axes$ size maybe not same as input\n",
    "\n",
    "+ Its $depth$ can be __arbitrary__, because the output depth is a parameter of the\n",
    "    layer, and the different channels in that __depth axis__  __no longer__ stand for specific __colors__\n",
    "    as in RGB input; rather, they stand for __filters__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Filters__ encode specific aspects of the\n",
    " input data: at a high level, a single filter could encode the concept “presence of a face\n",
    " in the input,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the MNIST example, the first convolution layer takes a feature map of size __(28,\n",
    "28, 1)__ and __outputs__ a feature map of size __(26, 26, 32)__: it computes __32 filters__ over its\n",
    "input. \n",
    "\n",
    "__Each__ of these 32 output channels __contains a 26 × 26 grid of values__, which is a\n",
    "__response map of the filter over the input__, indicating the response of that filter pattern at\n",
    "different locations in the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is what the term $feature$ $map$ means: \n",
    "\n",
    "Every dimension in the __depth axis__ is a __feature\n",
    "(or filter)__ , and the $nth$ __rank-2 tensor output__ $[:, :, n]$ is the __2D spatial map__ of the response\n",
    "of this filter over the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Convolutions are defined by two key parameters:\n",
    "+ $Size$ $of$ $the$ $patches$ $extracted$ $from$ $the$ $inputs$— These are typically 3 × 3 or 5 × 5. In the\n",
    "example, they were 3 × 3, which is a common choice.\n",
    "\n",
    "+ $Depth$ $of$ $the$ $output$ $feature$ $map$— This is the number of filters computed by the convolution. The example started with a depth of 32 and ended with a depth of 64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras Conv2D layers, these parameters are the first arguments passed to the layer:\n",
    "\n",
    "\n",
    "__Conv2D(output_depth, (window_height, window_width))__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolution works by _sliding_ these windows of size 3 × 3 or 5 × 5 over the 3D\n",
    " input feature map, stopping at every possible location, and extracting the 3D patch o \n",
    " surrounding features __(shape (window_height, window_width, input_depth))__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each such 3D patch is then transformed into a __1D vector__ of __shape (output_depth,)__ , which is\n",
    "done via a tensor product with a learned weight matrix\n",
    "\n",
    "It is called the $convolution$ $kernel$ —\n",
    "the same kernel is reused across every patch.\n",
    "\n",
    "All of these vectors (one per patch) are\n",
    " then __spatially__ reassembled into a __3D__ output map of shape = ___(height, width, output_\n",
    " depth)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every spatial location in the output feature map corresponds to the same\n",
    " location in the input feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output width and height may differ from the input width and height for\n",
    "two reasons:\n",
    "\n",
    "1.  __Border effects__, which can be countered by __padding__ the input feature map\n",
    "\n",
    "2.  The use of strides, which I’ll define in a second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand Border effect and padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a 5 × 5 feature map (25 tiles total). There are only 9 tiles around which you\n",
    "can center a 3 × 3 window, forming a 3 × 3 grid. \n",
    "\n",
    "\n",
    "Hence, the output feature map will be 3 × 3. It shrinks a little: by exactly two tiles alongside each dimension,\n",
    "in this case. \n",
    "\n",
    "You can see this border effect in action in the earlier example: you start\n",
    "with 28 × 28 inputs, which become 26 × 26 after the first convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get an output feature map with the same spatial dimensions as the\n",
    " input, you can use $padding$:  \n",
    " \n",
    " $Padding$ consists of __adding an appropriate number of rows and columns on each side of the input feature map__ so as to make it possible to fit center convolution windows around every input tile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In __Conv2D()__ layers, padding is configurable via the padding argument, which takes two\n",
    " values: \n",
    " \n",
    " 1. __\"valid\"__, which means no padding (only valid window locations will be used),\n",
    " \n",
    " 2. __\"same\"__, which means “pad in such a way as to have an output with the same width\n",
    " and height as the input.” \n",
    " \n",
    " The padding argument defaults to \"valid\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding Convolution Strides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other factor that can influence output size is the notion of __strides__. \n",
    "\n",
    "Our description  of convolution so far has assumed that the center tiles of the convolution windows are\n",
    " all contiguous.\n",
    "\n",
    "$stride$: __distance between two successive windows__ , default = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$strided$ $convolutions$: convolutions with a $stride$ higher than 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $stride$= 2 means the $width$ and $height$ of the feature map are __downsampled__ by a\n",
    " __factor of 2__ (in addition to any changes induced by border effects). \n",
    " \n",
    "Strided convolutions are rarely used in classification models, but they come in handy for some types o \n",
    " models, as you will see in the next chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2 The max-pooling operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __role__ of max pooling: To aggressively __downsample__ feature maps, much like\n",
    " strided convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s conceptually similar to convolution,\n",
    " except that __instead of transforming local patches__ via a learned linear transformation (the convolution kernel), they’re __transformed via a hardcoded max tensor\n",
    " operation__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big difference from convolution is that \n",
    "\n",
    "+  max pooling is usually done\n",
    " with __2 × 2__ windows and __stride 2__, in order to downsample the feature maps by a factor of 2. \n",
    " \n",
    "+  On the other hand, __convolution__ is typically done with __3 × 3__ windows and __no\n",
    " stride (stride 1)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why downsample feature maps this way? Why not remove the max-pooling layers\n",
    "and keep fairly large feature maps all the way up? Let’s look at this option. Our model\n",
    "would then look like the following listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 8.5 An incorrectly structured convnet missing its max-pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(28,28,1))\n",
    "\n",
    "x = layers.Conv2D(filters=32,kernel_size=3,activation='relu')(inputs)\n",
    "x = layers.Conv2D(filters=64,kernel_size=3,activation='relu')(x)\n",
    "x = layers.Conv2D(filters=128,kernel_size=3,activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "outputs = layers.Dense(10,activation='softmax')(x)\n",
    "\n",
    "model_no_max_pool = keras.Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 22, 22, 128)       73856     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 61952)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                619530    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 712,202\n",
      "Trainable params: 712,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_no_max_pool.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is __TWO__ problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It isn’t conducive to learning a spatial hierarchy of features. The __3 × 3__ windows\n",
    "in the __third layer__ will only contain information coming from __7 × 7__ windows in\n",
    "the __initial input__. \n",
    "The high-level patterns learned by the convnet will still be very\n",
    "small with regard to the initial input, which may __not be enough__ to learn to classify digits (try recognizing a digit by only looking at it through windows that are\n",
    "7 × 7 pixels!). We need the features from the last convolution layer to contain\n",
    "information about the totality of the input.\n",
    "\n",
    "\n",
    "2. The final feature map has 22 × 22 × 128 = __61,952 total coefficients per sample__.\n",
    "This is huge. When you flatten it to stick a Dense layer of size 10 on top, that\n",
    "layer would have over half a million parameters. __This is far too large for such a\n",
    "small model and would result in intense overfitting__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reason of using downsampling :\n",
    "\n",
    "1. __Reduce__ the number of feature-map coefficients to process\n",
    "\n",
    "2. __Induce__ spatial-filter hierarchies by making successive convolution layers look at increasingly large windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative : \n",
    "\n",
    "1. Use strides in the prior convolution layer. \n",
    "\n",
    "2. Use average pooling instead of max pooling, where each local input patch is transformed by taking the average value of each channel over the patch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Max pooling__ tends to work __better__ than these alternative solutions. \n",
    "\n",
    "The reason is that :\n",
    "\n",
    "1. Features tend to encode the spatial presence of some pattern or concept\n",
    " over the different tiles of the feature map.\n",
    "\n",
    "2. it’s more\n",
    " informative to look at the maximal presence of different features than at their average\n",
    " presence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __most reasonable subsampling strategy__ \n",
    "\n",
    "1. Produce dense maps of features (via unstrided convolutions)  \n",
    "\n",
    "2. Look at the maximal activation of the features over small patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e450050b432e843bda3c41bf3272c133bfc370a7003f3e377e27f87a49ce1127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
